**Online anomaly detection in quantum device calibration streams**

---

### 1. Problem

Superconducting and other quantum processors require frequent calibration to maintain acceptable coherence times and gate fidelities. Each calibration cycle exposes a collection of hardware parameters—such as T₁, T₂, single- and two-qubit gate error rates, and readout errors—that evolve over time due to noise, temperature fluctuations, crosstalk, and hardware ageing.

In practice, these calibration parameters are often:

- monitored one metric at a time,
- inspected using static tolerance bands (e.g., “alert if T₁ < threshold”),
- visualised via dashboards that provide limited support for multivariate analysis or adaptive behaviour.

This raises a simple question:

> **Can a lightweight multivariate online anomaly detector provide a more adaptive and informative view of quantum calibration drift than static, metric-wise thresholds?**

This repository provides a small, reproducible prototype to explore that question on both **synthetic calibration streams** and **real calibration snapshots** obtained via Qiskit.

---

### 2. Method

#### 2.1 Data: synthetic and real calibration streams

The project treats quantum device calibration as a **multivariate time series**. Each timestamp is a vector of calibration parameters across multiple qubits.

Two data sources are supported:

- **Synthetic calibration data**  
  - Generated by `src/generate_calibration_data.py`.  
  - Simulates T₁, T₂, gate error, and readout error for a configurable number of qubits.  
  - Injects controlled anomalies (e.g., sudden drops in T₁/T₂ or spikes in error rates) to stress-test detectors and support visual inspection.

- **Real calibration snapshots (Qiskit)**  
  - Collected by `src/fetch_real_calibration_data.py`.  
  - Uses `qiskit-ibm-runtime` to query an IBM Quantum backend for:
    - T₁ and T₂ per qubit,
    - readout error per qubit,
    - single-qubit gate error for a chosen gate (e.g., `x`).  
  - Each execution appends a new timestamped row to `data/real_calibration_stream.csv`, allowing users to build a real calibration log over time (e.g., by running the script periodically).

Both sources use the same downstream anomaly detection and visualisation pipeline.

---

#### 2.2 Online anomaly detector (Mahalanobis)

The main detector is implemented in `src/online_model.py` and is designed to be:

- **Online/streaming**: processes the calibration time series sequentially;
- **Multivariate**: accounts for correlations across calibration parameters;
- **Lightweight**: suitable for rapid prototyping and potential integration.

Core components:

1. **Sliding window** of recent calibration vectors (size configurable).
2. Estimation of **mean** and **covariance** on that window.
3. Computation of the **squared Mahalanobis distance** of the latest observation from the estimated distribution.
4. Maintenance of a short history of Mahalanobis distances and computation of an **adaptive threshold** as a high percentile (based on a target contamination rate).

A point is flagged as anomalous if its Mahalanobis distance exceeds this adaptive threshold. This yields an online, data-driven boundary between “background” calibration variation and atypical behaviour.

---

#### 2.3 Baseline detectors

To contextualise the behaviour of the online multivariate model, we implement two simple baselines on a selected scalar metric (e.g., `T1_q0`):

1. **Static ±k·σ threshold**  
   - Given an empirical mean μ and standard deviation σ of the metric, a point is flagged if it lies outside `[μ − kσ, μ + kσ]`.  
   - The parameter `k` can be adjusted in the UI.

2. **|z|-score threshold**  
   - The metric is normalised to a z-score `z = (x − μ) / σ`.  
   - A point is flagged if `|z|` exceeds a given threshold `z_thresh`.

These baselines represent typical “rule-of-thumb” monitoring strategies and serve as comparison points for the online Mahalanobis detector in terms of:

- how often alerts are raised (false alarm characteristics),
- whether certain anomalies are detected earlier or exclusively by one approach.

---

### 3. Demo

The main entry point for interactive exploration is the **Streamlit app** `src/app.py`, which provides an end-to-end demonstration of the anomaly detection pipeline.


#### Running the demo

 **Install dependencies**

```bash
git clone https://github.com/freelansire/Quantum-Calib-Anomaly.git
cd Quantum-Calib-Anomaly

pip install -r requirements.txt
```

### How to Cite

If you use **Online Anomaly Detection in Quantum Device Calibration Streams** in research, teaching, or derivative work, please cite the repository.

#### Suggested citation (APA-style)

Orokpo, S.M.(2025). Online Anomaly Detection in Quantum Device Calibration Streams   (Version 0.1.0) [Software]. GitHub. https://github.com/freelansire/Quantum-Calib-Anomaly
  
#### BibTeX

```bibtex
@software{orokpo_quantum_calibration_streams_2025,
  author       = {Samuel Moses Orokpo},
  title        = {Online Anomaly Detection in Quantum Device Calibration Streams},
  year         = {2025},
  version      = {1.0.1},
  url          = {https://github.com/freelansire/Quantum-Calib-Anomaly},
```
